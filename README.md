# Big_O

* Big O notation: (How long an algorithm takes to run?)
* Is a mathematical notation used in computer science and mathematics to describe the WORST case performance of an algorithm.

* It is used to analyze the TIME COMPLEXITY or SPACE COMPLEXITY of algorithms, in other words, is used to express how the runtime
or space requirements of an algorithm grow as the size of the input data (n) increases.

* It provides a way to compare and classify algorithms based on their efficiency.

* It provides a high-level understanding of algorithm efficiency, helping developers choose the right algorithms for their specific needs,
 and make right decisions about code optimization.
 
* Different types of Big Os:

1. O(1): Constant time Complexity
whe the algorithm's performance does not depend on the size of the input.
It is called constant time complexity with order O(1). This means that 
the run time will always be the same regardless of the size of the input.


(NO LOOPS)


2. O(n): Linear time complexity, the performance grows linearly with the input size.
(FOR LOOPS)
(WHILE LOOPS)

3. O(n^2): Quadratic time complexity, every element in a collection needs to be compared to every other element.
-(NESTED LOOPS)